Below is a **fully detailed, end-to-end explanation** of the paper **“An LLM-based Framework for Fingerprinting Internet-connected Devices” (IMC ’23)**. 

---

## What problem the paper solves

Internet-wide scanners (e.g., Censys/Shodan) collect **“banners”**: raw text returned by services during protocol handshakes (HTTP headers, auth prompts, cookies, etc.). These banners are extremely useful for figuring out **what device/software is running**, but:

* The data is **raw and messy** (tons of text, mixed formats, sometimes binary-like bytes).
* Labels are **incomplete**: many banners don’t match existing hand-curated fingerprint databases.
* Banners often contain **dynamic noise** that changes on every scan (timestamps, random IDs, session cookies), which makes naive text matching unstable.

So the authors want a system that can:

1. learn from banner text directly (no handcrafted features),
2. create **stable numeric embeddings** for banners that don’t drift when only dynamic fields change,
3. cluster similar banners, and
4. automatically generate **text fingerprints** (regex patterns) to label devices/software—especially those missing from existing fingerprint databases like **Recog**. 

---

## The high-level idea (pipeline)

The paper’s pipeline (illustrated in Figure 1) is:

1. **Collect banners** (mostly HTTP in the experiments).
2. Train a transformer language model to understand banner text.
3. Fine-tune it so that the resulting **banner embeddings** are:

   * **stable over time** for the same host/service,
   * **robust to dynamic parts** of banners,
   * and still separate different services/products.
4. Use embeddings to **cluster** banners (HDBSCAN).
5. From each cluster, automatically extract **common substrings** and convert them into **regex fingerprints**.
6. Compare generated fingerprints to **Recog**, showing discovery of new device/software patterns not in Recog. 

---

## Data: what they trained on

### Source and time window

They use **6 monthly snapshots** from the **Censys Universal Internet Dataset**, specifically the first Tuesday of each month from **July–December 2023**. 

### Scale

* Censys scans the full IPv4 space across many protocols/ports.
* They keep **non-empty, non-truncated** banners.
* They take a **10% subsample of IPs**, but keep the *same IP selection across months* so they can compare the same hosts over time.
* Result: **~260 million banners** total across snapshots. 

### Why exclude “truncated” services?

Censys flags “truncated” services (often hosts with >100 services) that are likely honeypots or weird/firewalled cases; they exclude these to improve quality. 

---

## Tokenization: why they retrain it

Transformers need tokenized input. Banners can contain unusual strings and even byte-ish data, so normal NLP tokenizers can produce many unknown tokens.

They choose **RoBERTa’s byte-level BPE tokenizer** approach because it is **lossless** (it can fall back to single bytes, avoiding unknown tokens). But they *don’t* use RoBERTa’s pretrained tokenizer directly, because scan banners differ from natural language.

So they **retrain a tokenizer**:

* vocabulary size: **50,000 tokens**
* trained on **100 million randomly selected banners** 

This matters a lot: if tokenization mangles weird strings (product IDs, cookie formats, firmware strings), embedding quality suffers.

---

## Model training part 1: masked language model (MLM)

### What is MLM doing here?

A masked language model learns to predict missing tokens from context. It forces the transformer to learn relationships like:

* “Server: nginx” often co-occurs with certain header patterns
* “WWW-Authenticate: Basic realm=…” has device/firmware implications
* Specific cookie names imply specific apps (grafana_sess → Grafana)

### Architecture (small RoBERTa-style transformer)

They train a RoBERTa model with:

* **256-d embeddings**
* **4 transformer layers**
* **4 attention heads**
* intermediate hidden size **1024**
* train for **100k iterations**
* batch size **1024**
* learning rate **2e-4**
* mask **15%** of tokens 

They pretrain on **all protocols**, but the rest of the paper focuses on **HTTP banners** (~70% of their dataset). 

---

## The key contribution: making embeddings temporally stable

### Why stability is hard

Banners contain both:

* **stable identity features** (product name, server type, auth realm, firmware family),
* **dynamic noise** (timestamps, nonces, session IDs, randomized tokens, some version subparts).

If you embed the whole banner naively, the embedding can shift because the session ID changed, even though the device is identical.

### Their approach: learn what’s “stable” vs “dynamic” using time-separated pairs

They create **banner pairs**:

* Same **IP address + port**
* From **two consecutive monthly snapshots**

This strongly suggests “same underlying service/device” most of the time, but it can still fail (IP churn, service replaced). They filter pairs to reduce mismatches.

#### Step A: find stable substrings in paired banners

For each banner pair, they:

1. Split HTTP banners into **headers** first (important: avoids matching across different header fields, and is robust to header reordering).
2. Do **common substring matching** per-header to find overlapping parts.
3. Ignore tiny matches (<3 characters).
4. Discard pairs if total matched content is too small (<11 tokens), because that suggests the “pair” might actually be different devices/services. 

This produces token-level labels:

* 1 = “matched across months” (stable-ish)
* 0 = “not matched” (dynamic-ish)

#### Step B: train a token classifier to predict stable/dynamic tokens

They fine-tune the transformer as a **token classification** model that predicts whether each token is stable.

This is important because it gives interpretability: you can literally highlight which parts of a banner the model believes are dynamic. The paper shows an example (Figure 2) where timestamps and changing fields are grayed out. 

---

## How they compute the banner embedding (core formula)

They use token embeddings from the last transformer layer, but instead of averaging all tokens equally, they do a **weighted average**:

* Each token embedding ( e_{i,j}^{(k)} ) is weighted by the model’s predicted “stability probability” ( \hat{y}_{i,j}^{(k)} ).
* Tokens predicted dynamic (near 0) contribute almost nothing.
* Tokens predicted stable (near 1) dominate.

Then they **ℓ2-normalize** the resulting vector to unit norm. 

Intuition:

* The model learns to “ignore” timestamps, random cookie IDs, nonces, etc.
* The embedding becomes more like “what product/service is this?” rather than “what was the exact banner string at scan time?”

---

## The loss function: token stability + contrastive embedding stability

Their fine-tuning objective combines three forces (Equation 1):

1. **Token classification loss** (binary cross-entropy):

   * predict which tokens are stable vs dynamic.

2. **Pull matching pairs together**:

   * minimize distance between embeddings of the same IP/port across two months.

3. **Push non-matching pairs apart**:

   * maximize distance between embedding of banner i and a “shifted” mismatched partner (circular shift trick). 

Why ℓ2-normalization matters:

* If you push embeddings apart without normalization, the model could “cheat” by just increasing vector magnitudes indefinitely. Unit norm prevents that.

### Training details

They fine-tune for:

* **20k iterations**
* batch size **1024** (512 pairs)
* learning rate **5e-5** 

---

## Results: does it actually learn stable vs dynamic?

### Token labeling performance

On a held-out test set (100k pairs), token classification gets:

* accuracy **98.3%**
* positive-label precision/recall ~**96.9% / 98.9%**
* negative-label precision/recall ~**99.3% / 97.9%** 

### Qualitative example (Figure 2)

They show a banner where:

* timestamps and a changing header are correctly marked dynamic,
* even a **minor version tail** is predicted dynamic because it tends to change between snapshots (e.g., patch version updates). 

This is subtle: the model isn’t just deleting “obvious” dynamic things; it learns *statistical regularities of change* across time.

---

## Embedding quality: matching vs random separation

They evaluate distances between:

* “matching” pairs (same IP/port across months),
* random pairs.

### Key finding: contrastive fine-tuning massively separates these distributions

They compare:

* vanilla MLM embeddings
* contrastive (their) embeddings

With contrastive training, a strong threshold emerges around distance ~0.1:

* **97.0%** of matching pairs have distance < 0.1
* **97.8%** of random pairs have distance > 0.1 

They also note many pairs have distance **0** (perfectly identical banners), especially at one-month spacing (~52.3%). Over five months, perfect matches drop (16.6%), but “distance < 0.1” stays high (94.7%), showing long-window stability. 

### Why this matters

This suggests embeddings can be used for:

* **change detection** (when distance spikes, something major changed),
* robust clustering (similar products/services are near each other).

They give examples (Figure 4) where large embedding distances correspond to major configuration changes or different server products. 

---

## Clustering: how they group similar services

### Dimensionality reduction (PCA)

They reduce 256 → 64 dimensions with PCA:

* trained on **5 million embeddings**
* explained variance retained: **99.97%** 

This is mainly for computational efficiency before clustering.

### Clustering algorithm: HDBSCAN

They use **HDBSCAN**, a hierarchical density-based clustering algorithm.

Why HDBSCAN (vs k-means)?

* Doesn’t require number of clusters.
* Handles clusters of varying shape/density.
* Good for real-world messy embedding spaces.

Training setup:

* 5 million embeddings
* min_cluster_size = 50
* min_samples = 5
* they vary `cluster_selection_epsilon` in {0.01, 0.02, 0.05, 0.1} to get different cluster granularities. 

As epsilon increases, clusters merge more, and outliers decrease. They report cluster counts and outlier % for each setting. 

---

## Fingerprint generation: turning clusters into regex patterns

This is the practical “label the scan data” part.

### For each cluster:

1. Randomly select **10 samples** from that cluster.
2. Do **longest common substring matching** across those samples, per-header.
3. Convert common substrings into a **regex**:

   * fixed substrings + wildcard segments
   * produce one regex per header
4. Repeat with **100 different random sets of 10 samples** per cluster.
5. Do this across all 4 clustering granularities. 

Output:

* **15,718 regex patterns** across headers. 

So the system produces a large candidate fingerprint pool.

---

## Comparison with Recog: what’s new and what’s recovered

Recog is a hand-curated fingerprint database used widely (including by Censys) to label services.

They focus on three HTTP headers (because these are very fingerprint-informative):

* Server
* Set-Cookie
* WWW-Authenticate 

### They find fingerprints not captured by Recog

They compare counts:

* Recog has: 447 (Server), 82 (Set-Cookie), 77 (WWW-Authenticate)
* Their system generates far more candidates for those headers (e.g., 798 / 2478 / 635 in the same order). 

They show examples (Table 1) of new fingerprints, including IoT devices and server software, like:

* Brovotech IP Camera via Server header regex
* Grafana via cookie name
* Sagemcom Livebox router via WWW-Authenticate realm
* Hanwha Wisenet NVR via Digest realm pattern 

### How well do they “recover” Recog?

They also try to match their fingerprints to Recog fingerprints using an **overlap ratio** (how many banners match both vs either).

They report:

* exact-ish recovery (>=90% overlap): recovers 117 / 9 / 22 Recog fingerprints (Server / Set-Cookie / WWW-Authenticate)
* these recovered patterns cover a very high fraction of the banners that Recog labels (e.g., 98.1% coverage for Server). 

They also look at partial matches (subset patterns) and get even higher banner coverage. 

Interpretation:

* Their method is very good at recovering **frequent** Recog patterns.
* Less frequent patterns are harder; improving that is future work.

---

## What’s genuinely novel here

1. **Train on raw banner text directly**

   * Previous approaches often build bag-of-words or other intermediate features.
   * Here the transformer learns directly from raw strings.

2. **Temporal supervision for stability**

   * They exploit monthly snapshots to learn what changes vs what stays.
   * The weighted embedding aggregation is tightly coupled to this.

3. **Interpretability via token annotations**

   * Transformers allow mapping predictions back to exact banner substrings (Figure 2).
   * That’s useful for practitioners debugging why a fingerprint/cluster exists.

4. **Automated fingerprint generation at scale**

   * Clustering + substring extraction → regex patterns that can complement manual databases.

---

## Limitations and caveats (the paper is honest about these)

1. **Regex fingerprints still need manual sanitization**

   * Patterns can include configuration artifacts (like `path=/.*` in Set-Cookie examples).
   * Near-duplicates can exist and must be deduplicated. 

2. **Quality depends on “matching pairs” quality**

   * They assume same IP/port across months often means same device/service, but IP churn and service swaps happen.
   * They filter pairs with low overlap, but it’s not perfect. 

3. **Focus is mainly HTTP in evaluation**

   * They trained MLM across protocols, but fingerprint generation results are shown for HTTP.
   * They suggest applying the same methodology to other protocols where fingerprints are sparse. 

4. **This is a short paper / preliminary exploration**

   * They show feasibility and strong initial results, but not an exhaustive end-to-end deployment/maintenance story.

---

## How you should think about the contribution (practical intuition)

If you’ve ever tried to label scan data, you know the pain:

* exact string matching breaks,
* small banner changes cause false “new device” detections,
* fingerprints lag behind reality.

This paper essentially builds an **“embedding layer”** that:

* collapses away randomness (cookies, timestamps),
* preserves identity signals (server/app/device strings),
* enables clustering and fingerprint suggestion.

So, instead of humans hand-curating every new fingerprint, the model can:

* surface “here’s a new cluster that looks like a device family”
* propose regex candidates for a human to validate.
